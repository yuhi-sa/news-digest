"""Article summarization (pluggable strategy)."""

from __future__ import annotations

import html
import json
import logging
import re
import time
import urllib.request
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import replace

from .parser import Article

logger = logging.getLogger(__name__)

_MAX_BODY_CHARS = 10000


def _fetch_page_text(url: str, timeout: int = 15) -> str:
    """Fetch a URL and return plain text extracted from HTML.

    Returns empty string on any failure.
    """
    try:
        req = urllib.request.Request(
            url,
            headers={"User-Agent": "NewsDigestBot/1.0"},
        )
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            raw = resp.read().decode("utf-8", errors="replace")
    except Exception:
        logger.debug("Failed to fetch %s", url)
        return ""

    # Remove script/style blocks, then strip all tags
    text = re.sub(r"<(script|style)[^>]*>.*?</\1>", "", raw, flags=re.S | re.I)
    text = re.sub(r"<[^>]+>", " ", text)
    text = html.unescape(text)
    text = re.sub(r"\s+", " ", text).strip()
    return text[:_MAX_BODY_CHARS]


def _fetch_pages_parallel(
    urls: list[str], max_workers: int = 6,
) -> dict[str, str]:
    """Fetch multiple URLs in parallel. Returns {url: text}."""
    results: dict[str, str] = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {
            executor.submit(_fetch_page_text, url): url for url in urls
        }
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                results[url] = future.result()
            except Exception:
                results[url] = ""
    return results

_PROMPT_TEMPLATE = (
    "ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã‚’èª­ã‚“ã§ã€æ—¥æœ¬èªã§1ã€œ2æ–‡ã®ç°¡æ½”ãªè¦ç´„ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"
    "è¦ç´„ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
    "ã‚¿ã‚¤ãƒˆãƒ«: {title}\n"
    "æ¦‚è¦: {summary}"
)

_BATCH_PROMPT_TEMPLATE = (
    "ä»¥ä¸‹ã®è¤‡æ•°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã«ã¤ã„ã¦ã€ãã‚Œãã‚Œæ—¥æœ¬èªã§1ã€œ2æ–‡ã®ç°¡æ½”ãªè¦ç´„ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n"
    "å„è¦ç´„ã¯ç•ªå·ä»˜ãã§è¿”ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 1. è¦ç´„æ–‡ï¼‰ã€‚\n"
    "è¦ç´„ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
    "{articles}"
)


class Summarizer(ABC):
    """Base class for article summarizers."""

    @abstractmethod
    def summarize(self, articles: list[Article]) -> list[Article]:
        """Return articles with potentially updated summaries."""


class PassthroughSummarizer(Summarizer):
    """Uses RSS description as-is (no external API calls)."""

    def summarize(self, articles: list[Article]) -> list[Article]:
        logger.info("PassthroughSummarizer: keeping original summaries for %d articles", len(articles))
        return articles


GEMINI_ENDPOINT = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"


def call_gemini(prompt: str, api_key: str, max_retries: int = 2) -> str | None:
    """Call Gemini API with retry logic and return the generated text.

    Retries up to max_retries times on failure with backoff.
    """
    url = f"{GEMINI_ENDPOINT}?key={api_key}"
    payload = json.dumps({
        "contents": [{"parts": [{"text": prompt}]}],
    }).encode("utf-8")

    for attempt in range(max_retries + 1):
        req = urllib.request.Request(
            url,
            data=payload,
            headers={"Content-Type": "application/json"},
            method="POST",
        )
        try:
            with urllib.request.urlopen(req, timeout=60) as resp:
                data = json.loads(resp.read().decode("utf-8"))
            return data["candidates"][0]["content"]["parts"][0]["text"].strip()
        except Exception:
            if attempt < max_retries:
                delay = 5 * (attempt + 1)
                logger.warning(
                    "Gemini API call failed (attempt %d/%d), retrying in %ds",
                    attempt + 1, max_retries + 1, delay,
                )
                time.sleep(delay)
            else:
                logger.exception(
                    "Gemini API call failed after %d attempts", max_retries + 1,
                )
    return None


class GeminiSummarizer(Summarizer):
    """Summarizes articles in Japanese using Google Gemini API (free tier)."""

    ENDPOINT = GEMINI_ENDPOINT

    def __init__(self, api_key: str):
        self.api_key = api_key

    def _call_gemini(self, prompt: str) -> str | None:
        """Call Gemini API and return the generated text."""
        return call_gemini(prompt, self.api_key)

    def _summarize_single(self, article: Article) -> Article:
        """Summarize a single article via Gemini API."""
        prompt = _PROMPT_TEMPLATE.format(title=article.title, summary=article.summary)
        ja_summary = self._call_gemini(prompt)
        if ja_summary:
            return replace(article, summary=ja_summary)
        logger.warning("Fallback to original summary for: %s", article.title)
        return article

    def _summarize_batch(self, batch: list[Article]) -> list[Article]:
        """Summarize a batch of articles in a single API call.

        Falls back to individual calls if the batch call fails.
        """
        articles_text = "\n".join(
            f"{i + 1}. ã‚¿ã‚¤ãƒˆãƒ«: {a.title}\n   æ¦‚è¦: {a.summary}"
            for i, a in enumerate(batch)
        )
        prompt = _BATCH_PROMPT_TEMPLATE.format(articles=articles_text)
        response = self._call_gemini(prompt)

        if response:
            summaries = self._parse_batch_response(response, len(batch))
            if summaries:
                results: list[Article] = []
                for article, summary in zip(batch, summaries):
                    results.append(replace(article, summary=summary))
                return results

        # Fallback: summarize individually
        logger.warning("Batch summarization failed, falling back to individual calls for %d articles", len(batch))
        return [self._summarize_single(a) for a in batch]

    @staticmethod
    def _parse_batch_response(response: str, expected_count: int) -> list[str] | None:
        """Parse numbered summaries from a batch response.

        Returns None if parsing fails or count doesn't match.
        """
        import re
        lines = response.strip().split("\n")
        summaries: list[str] = []
        current = ""
        for line in lines:
            match = re.match(r"^\d+[\.\)]\s*", line)
            if match:
                if current:
                    summaries.append(current.strip())
                current = line[match.end():]
            else:
                if current:
                    current += " " + line.strip()
        if current:
            summaries.append(current.strip())

        if len(summaries) == expected_count:
            return summaries
        logger.warning(
            "Batch response parse mismatch: expected %d, got %d",
            expected_count,
            len(summaries),
        )
        return None

    def summarize(self, articles: list[Article], batch_size: int = 5) -> list[Article]:
        logger.info("GeminiSummarizer: summarizing %d articles in Japanese (batch_size=%d)", len(articles), batch_size)
        results: list[Article] = []
        for i in range(0, len(articles), batch_size):
            batch = articles[i : i + batch_size]
            results.extend(self._summarize_batch(batch))
        return results

    # ------------------------------------------------------------------
    # Two-stage briefing
    # ------------------------------------------------------------------

    def _select_articles(self, articles: list[Article]) -> list[int]:
        """Stage 1: Ask Gemini to pick the most important article indices."""
        article_list = "\n".join(
            f"{i}. [{a.category}] {a.title}: {a.summary}"
            for i, a in enumerate(articles)
        )
        prompt = (
            "ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å…¼æ—¥æœ¬æ ªãƒ»ç±³å›½æ ªã®å€‹äººæŠ•è³‡å®¶å‘ã‘ã®"
            "ã‚·ãƒ‹ã‚¢ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚\n"
            "ä»¥ä¸‹ã®è¨˜äº‹ä¸€è¦§ã‹ã‚‰ã€èª­è€…ã«ã¨ã£ã¦æœ¬å½“ã«é‡è¦ãªè¨˜äº‹ã‚’**8ã€œ10ä»¶**é¸ã‚“ã§ãã ã•ã„ã€‚\n\n"
            "## èª­è€…ã®æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯\n"
            "èª­è€…ã¯ä»¥ä¸‹ã®æŠ€è¡“ã‚’æ—¥å¸¸çš„ã«ä½¿ã†ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚"
            "ã“ã‚Œã‚‰ã«é–¢é€£ã™ã‚‹è¨˜äº‹ã¯å„ªå…ˆçš„ã«é¸ã‚“ã§ãã ã•ã„:\n"
            "- è¨€èª: TypeScript/Next.js, Python, Go, Spark\n"
            "- ã‚¤ãƒ³ãƒ•ãƒ©: Kubernetes, Kafka, MySQL, Cassandra, Redis, Hadoop, Athenz\n"
            "- ãƒ‡ãƒ¼ã‚¿åŸºç›¤: dbt, Airflow, Databricks, BigQuery, Athena\n\n"
            "## å¿…é ˆã®é¸å®šé…åˆ†\n"
            "ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«æœ€ä½é™ã®è¨˜äº‹æ•°ã‚’ç¢ºä¿ã™ã‚‹ã“ã¨:\n"
            "- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: 3ã€œ5ä»¶ï¼ˆå®Ÿéš›ã«æ‚ªç”¨ã•ã‚Œã¦ã„ã‚‹CVEã€é‡å¤§ãªè„†å¼±æ€§ã€æ”»æ’ƒã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã®ã¿ã€‚"
            "ä¸€èˆ¬è«–ã‚„å•“è’™è¨˜äº‹ã¯é™¤å¤–ï¼‰\n"
            "- ãƒãƒ¼ã‚±ãƒƒãƒˆ/æŠ•è³‡: 2ã€œ3ä»¶ï¼ˆå…·ä½“çš„æ•°å€¤ãƒ»æŒ‡æ¨™ãƒ»æ±ºç®—ã‚’å«ã‚€è¨˜äº‹ã‚’å„ªå…ˆã€‚"
            "æ•°å­—ã®ãªã„ä¸€èˆ¬çš„ãªçµŒæ¸ˆè«–è©•ã¯é™¤å¤–ï¼‰\n"
            "- ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°: 1ã€œ3ä»¶ï¼ˆdbt/Airflow/Spark/BigQueryç­‰ã®å…·ä½“çš„ãƒ„ãƒ¼ãƒ«æ›´æ–°ãƒ»"
            "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´ã‚’å«ã‚€è¨˜äº‹ï¼‰\n"
            "- ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼å…¨èˆ¬: 3ã€œ5ä»¶ï¼ˆèª­è€…ã‚¹ã‚¿ãƒƒã‚¯ã«ç›´çµã™ã‚‹è¨˜äº‹ã‚’å„ªå…ˆï¼‰\n\n"
            "## é¸å®šåŸºæº–ï¼ˆå„ªå…ˆé †ï¼‰\n"
            "1. å…·ä½“çš„ãªæ•°å€¤ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»CVEç•ªå·ã‚’å«ã‚€è¨˜äº‹ã‚’æœ€å„ªå…ˆ\n"
            "2. ä¸Šè¨˜ã‚¹ã‚¿ãƒƒã‚¯ã«é–¢é€£ã™ã‚‹é‡è¦ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãƒ»è„†å¼±æ€§ãƒ»ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n"
            "3. æŠ•è³‡åˆ¤æ–­ã«ç›´çµï¼ˆãƒã‚¯ãƒ­æŒ‡æ¨™ã®å…·ä½“æ•°å€¤ã€æ±ºç®—ã€ã‚»ã‚¯ã‚¿ãƒ¼å‹•å‘ï¼‰\n"
            "4. äº›æœ«ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã€å®£ä¼çš„ãªè¨˜äº‹ã€æ—¢çŸ¥ã®ç¹°ã‚Šè¿”ã—ã¯é™¤å¤–\n"
            "5. é‡ã‚ˆã‚Šè³ª: ä¼¼ãŸãƒ†ãƒ¼ãƒã®è¨˜äº‹ã¯æœ€ã‚‚æƒ…å ±é‡ã®å¤šã„1ä»¶ã ã‘é¸ã¶\n\n"
            "## å‡ºåŠ›å½¢å¼\n"
            "é¸ã‚“ã è¨˜äº‹ã®ç•ªå·ã‚’JSONé…åˆ—ã§è¿”ã—ã¦ãã ã•ã„ã€‚ãã‚Œä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸è¦ã§ã™ã€‚\n"
            "ä¾‹: [0, 3, 5, 7, 9, 12, 15, 18]\n\n"
            f"## è¨˜äº‹ä¸€è¦§ï¼ˆ{len(articles)}ä»¶ï¼‰\n\n"
            f"{article_list}"
        )
        logger.info("Stage 1: selecting important articles from %d candidates", len(articles))
        response = self._call_gemini(prompt)
        if not response:
            return []

        # Extract JSON array from response
        try:
            match = re.search(r"\[[\d\s,]+\]", response)
            if match:
                indices = json.loads(match.group())
                valid = [i for i in indices if 0 <= i < len(articles)]
                logger.info("Stage 1: selected %d articles", len(valid))
                return valid
        except (json.JSONDecodeError, ValueError):
            pass
        logger.warning("Stage 1: failed to parse selection response")
        return []

    _BRIEFING_MIN_CHARS = 200

    def generate_briefing(self, articles: list[Article]) -> str | None:
        """Generate a curated daily briefing using two-stage approach.

        Stage 1: Select important articles from RSS summaries.
        Stage 2: Fetch full text of selected articles, then generate deep briefing.
        Includes retry logic for empty or too-short results.
        """
        # Stage 1: Select
        selected_indices = self._select_articles(articles)
        if not selected_indices:
            logger.warning("Stage 1 returned no articles, falling back to summary-only briefing")
            selected = articles[:10]
        else:
            selected = [articles[i] for i in selected_indices]

        # Fetch full text of selected articles
        urls = [a.link for a in selected if a.link]
        logger.info("Stage 2: fetching full text for %d selected articles", len(urls))
        page_texts = _fetch_pages_parallel(urls)
        fetched = sum(1 for t in page_texts.values() if t)
        logger.info("Stage 2: successfully fetched %d/%d pages", fetched, len(urls))

        # Build enriched article list
        enriched_parts: list[str] = []
        for a in selected:
            body = page_texts.get(a.link, "")
            entry = (
                f"### [{a.category}] {a.title}\n"
                f"- URL: {a.link}\n"
                f"- RSSæ¦‚è¦: {a.summary}\n"
            )
            if body:
                entry += f"- è¨˜äº‹æœ¬æ–‡ï¼ˆæŠœç²‹ï¼‰: {body}\n"
            enriched_parts.append(entry)
        enriched_text = "\n".join(enriched_parts)

        # Stage 2: Generate briefing with full context
        prompt = (
            "ã‚ãªãŸã¯ãƒ™ãƒ†ãƒ©ãƒ³ã®ãƒ†ãƒƒã‚¯ã‚¸ãƒ£ãƒ¼ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å…¼"
            "å€‹äººæŠ•è³‡å®¶ï¼ˆæ—¥ç±³æ ªï¼‰å‘ã‘ã®ãƒ‡ã‚¤ãƒªãƒ¼ãƒ–ãƒªãƒ¼ãƒ•ã‚£ãƒ³ã‚°ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n"
            "## èª­è€…\n"
            "- æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯: TypeScript/Next.js, Python, Go, Spark, "
            "Kubernetes, Kafka, MySQL, Cassandra, Redis, Hadoop, Athenz, "
            "dbt, Airflow, Databricks, BigQuery, Athena\n"
            "- èª­è€…ã®ã‚¹ã‚¿ãƒƒã‚¯ã«ç›´çµã™ã‚‹è©±é¡Œã¯æŠ€è¡“åã‚’æŒ™ã’ã¦å½±éŸ¿ã‚’å…·ä½“çš„ã«è¿°ã¹ã‚‹\n"
            "- èª­è€…ã¯æ—¥ç±³ã®å€‹åˆ¥æ ªãƒ»ETFã«æŠ•è³‡ã—ã¦ã„ã‚‹ã€‚ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æŠ•è³‡ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’çŸ¥ã‚ŠãŸã„\n\n"
            "## ç¦æ­¢è¡¨ç¾ï¼ˆã“ã‚Œã‚‰ã‚’ä½¿ã£ãŸã‚‰æ›¸ãç›´ã™ï¼‰\n"
            "- ã€Œã€œã«æ³¨ç›®ãŒé›†ã¾ã£ã¦ã„ã¾ã™ã€ã€Œã€œãŒé‡è¦ã§ã™ã€ã€Œã€œãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€\n"
            "- ã€Œã€œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€ã§çµ‚ã‚ã‚‹æ–‡\n"
            "- ã€Œã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¯æ³¨æ„ãŒå¿…è¦ã§ã™ã€ã€Œå¯¾ç­–ãŒæ€¥å‹™ã§ã™ã€\n"
            "- ã€Œã€œãŒé€²ã‚“ã§ã„ã¾ã™ã€ã€Œã€œãŒåŠ é€Ÿã—ã¦ã„ã¾ã™ã€\n"
            "- ã€Œä»Šå¾Œã®å‹•å‘ã«æ³¨ç›®ã€ã€Œå¼•ãç¶šãæ³¨è¦–ã€\n"
            "- ã€Œã€œãŒæœŸå¾…ã•ã‚Œã¾ã™ã€ã€Œã€œãŒè¦‹è¾¼ã¾ã‚Œã¾ã™ã€ï¼ˆæ ¹æ‹ ãªã—ã®å ´åˆï¼‰\n"
            "- åŒã˜èªå°¾ã®3é€£ç¶šï¼ˆã€Œã€œã—ãŸã€‚ã€œã—ãŸã€‚ã€œã—ãŸã€‚ã€ã¯ä¸å¯ï¼‰\n\n"
            "## æ–‡ä½“\n"
            "- 1ãƒˆãƒ”ãƒƒã‚¯5ã€œ8è¡Œã€‚äº‹å®Ÿãƒ»èƒŒæ™¯ãƒ»èª­è€…ã¸ã®å½±éŸ¿ã‚’è¸ã¿è¾¼ã‚“ã§æ›¸ã\n"
            "- 1æ–‡ã¯40å­—ä»¥å†…ã€‚é•·ã„æ–‡ã¯åˆ†å‰²ã™ã‚‹\n"
            "- åŸºæœ¬æ§‹æˆ: äº‹å®Ÿ(1ã€œ2æ–‡) ï¼‹ æŠ€è¡“çš„èƒŒæ™¯(1ã€œ2æ–‡) ï¼‹ èª­è€…ã®æ¥­å‹™ã¸ã®å½±éŸ¿(1ã€œ2æ–‡)\n"
            "- å…¨ãƒˆãƒ”ãƒƒã‚¯ã®æœ«å°¾ã« ğŸ“ [è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«](URL) å¿…é ˆã€‚ä¾‹å¤–ãªã—\n"
            "- è¤‡æ•°ã®é–¢é€£è¨˜äº‹ã¯1ãƒˆãƒ”ãƒƒã‚¯ã«ã¾ã¨ã‚ã¦ã‚ˆã„\n"
            "- å„ãƒãƒ¬ãƒƒãƒˆãƒã‚¤ãƒ³ãƒˆã«ã¯å¿…ãš1ã¤ä»¥ä¸Šã®å…·ä½“çš„äº‹å®Ÿï¼ˆæ•°å€¤ã€å›ºæœ‰åè©ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã€"
            "CVEç•ªå·ãªã©ï¼‰ã‚’å«ã‚ã‚‹ã€‚å…·ä½“æ€§ã®ãªã„ãƒãƒ¬ãƒƒãƒˆã¯æ›¸ã‹ãªã„\n\n"
            "## ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆ\n\n"
            "### `## ğŸ”¥ æœ¬æ—¥ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ`\n"
            "æœ€é‡è¦ã®3ä»¶ã®ã¿ã€‚å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨é‡è¤‡ã—ãªã„ã“ã¨ã€‚\n"
            "- **å¤ªå­—è¦‹å‡ºã—**ï¼ˆ10å­—å‰å¾Œï¼‰\n"
            "- äº‹å®Ÿ1æ–‡ + æ„å‘³1æ–‡\n"
            "- ğŸ“ ãƒªãƒ³ã‚¯\n\n"
            "### `## ğŸ› ï¸ ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼`\n"
            "èª­è€…ã®æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ï¼ˆTypeScript, Python, Go, K8s, Kafkaç­‰ï¼‰ã«ç›´çµã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã®ã¿ã€‚\n"
            "ãƒã‚¤ãƒ©ã‚¤ãƒˆã¨é‡è¤‡ã—ãªã„åˆ¥ã®ãƒˆãƒ”ãƒƒã‚¯ã€‚æœ€å¤§3ä»¶ã€‚\n"
            "å…·ä½“çš„ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã€APIå¤‰æ›´ç‚¹ã€ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹é †ãŒã‚ã‚Œã°æ˜è¨˜ã€‚\n"
            "ğŸ“ ãƒªãƒ³ã‚¯å¿…é ˆã€‚\n\n"
            "### `## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°`\n"
            "ãƒ‡ãƒ¼ã‚¿åŸºç›¤ãƒ»ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–¢é€£ã€‚è©²å½“ãªã—ãªã‚‰çœç•¥ã€‚æœ€å¤§3ä»¶ã€‚\n"
            "dbt/Airflow/Spark/BigQuery/Databricksç­‰ã®å…·ä½“åã§å½±éŸ¿ã‚’è¿°ã¹ã‚‹ã€‚\n"
            "ãƒ„ãƒ¼ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€è¨­å®šå¤‰æ›´ç‚¹ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®å…·ä½“æ•°å€¤ã‚’å«ã‚ã‚‹ã€‚\n"
            "ğŸ“ ãƒªãƒ³ã‚¯å¿…é ˆã€‚\n\n"
            "### `## ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£`\n"
            "è„†å¼±æ€§ãƒ»æ”»æ’ƒå‹•å‘ã€‚è©²å½“ãªã—ãªã‚‰çœç•¥ã€‚**æœ€å¤§5ä»¶ã€å½±éŸ¿åº¦é †**ã€‚\n"
            "å„é …ç›®ã«å¿…é ˆ: (1)CVEç•ªå·ï¼ˆã‚ã‚Œã°ï¼‰, (2)å½±éŸ¿ã‚’å—ã‘ã‚‹ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³, "
            "(3)æ·±åˆ»åº¦ï¼ˆCritical/High/Mediumï¼‰, (4)å…·ä½“çš„å¯¾å¿œç­–ï¼ˆãƒ‘ãƒƒãƒé©ç”¨ã€è¨­å®šå¤‰æ›´ç­‰ï¼‰\n"
            "é¡ä¼¼ã®è„†å¼±æ€§ã¯1ãƒˆãƒ”ãƒƒã‚¯ã«ã¾ã¨ã‚ã‚‹ã€‚\n"
            "ğŸ“ ãƒªãƒ³ã‚¯å¿…é ˆã€‚\n\n"
            "### `## ğŸ“ˆ ãƒãƒ¼ã‚±ãƒƒãƒˆ`\n"
            "**è¨˜äº‹æœ¬æ–‡ã‹ã‚‰æŠ½å‡ºã—ãŸå…·ä½“çš„æ•°å€¤ã®ã¿è¨˜è¼‰**ã€‚ä»¥ä¸‹ã‚’å¯èƒ½ãªé™ã‚Šå«ã‚€:\n"
            "- æ ªä¾¡æŒ‡æ•°ï¼ˆS&P500, NASDAQ, æ—¥çµŒ225, TOPIXï¼‰ã®æ•°å€¤ã¨å‰æ—¥æ¯”%\n"
            "- ç‚ºæ›¿ï¼ˆUSD/JPYï¼‰ã®æ°´æº–\n"
            "- ç±³å›½å‚µåˆ©å›ã‚Šï¼ˆ10å¹´ï¼‰ã®æ°´æº–\n"
            "- å€‹åˆ¥éŠ˜æŸ„ã®æ±ºç®—ãƒ»æ ªä¾¡å¤‰å‹•ï¼ˆãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ä»˜ãï¼‰\n"
            "**è¨˜äº‹ã«æ•°å€¤ãŒãªã„å ´åˆã¯ã€Œãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼šè©²å½“è¨˜äº‹ã«å…·ä½“çš„æ•°å€¤ã®è¨˜è¼‰ãªã—ã€ã¨æ­£ç›´ã«æ›¸ãã€‚**\n"
            "æ•°å€¤ã‚’æé€ ãƒ»æ¨æ¸¬ã—ãªã„ã“ã¨ã€‚\n"
            "ğŸ“ ãƒªãƒ³ã‚¯å¿…é ˆã€‚\n\n"
            "### `## ğŸ”® ä»Šå¾Œã®æ³¨ç›®`\n"
            "1ã€œ2é€±é–“ä»¥å†…ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ»äºˆæ¸¬ã‚’2ã€œ3ç‚¹ã€‚**å…·ä½“çš„ãªæ—¥ä»˜ã‚’å¿…ãšæ˜è¨˜**ã€‚\n"
            "æ¼ ç„¶ã¨ã—ãŸäºˆæ¸¬ã¯æ›¸ã‹ãªã„ã€‚\n\n"
            "## ãƒ«ãƒ¼ãƒ«\n"
            "- è¨˜äº‹æœ¬æ–‡ã‚’è¸ã¾ãˆã¦æ›¸ãï¼ˆRSSæ¦‚è¦ã ã‘ã«é ¼ã‚‰ãªã„ï¼‰\n"
            "- ã€Œã ã‹ã‚‰ä½•ï¼Ÿã€ã‚’å¸¸ã«æ„è­˜ã€‚äº‹å®Ÿã®ç¾…åˆ—ã¯ä¸å¯\n"
            "- è¤‡æ•°è¨˜äº‹ã‚’æ¨ªæ–­çš„ã«çµã³ã¤ã‘ã¦ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æŠ½å‡º\n"
            "- ãƒã‚¤ãƒ©ã‚¤ãƒˆã®è¨˜äº‹ã¯ä»–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«æ›¸ã‹ãªã„ï¼ˆé‡è¤‡å³ç¦ï¼‰\n"
            "- å†’é ­æŒ¨æ‹¶ãƒ»æœ«å°¾ç· ã‚ä¸è¦ã€‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã ã‘å‡ºåŠ›\n"
            "- è¨˜äº‹ã«æ›¸ã‹ã‚Œã¦ã„ãªã„æ•°å€¤ã‚„äº‹å®Ÿã‚’æé€ ã—ãªã„\n\n"
            f"## å³é¸è¨˜äº‹ï¼ˆ{len(selected)}ä»¶ãƒ»æœ¬æ–‡ä»˜ãï¼‰\n\n"
            f"{enriched_text}"
        )
        logger.info("Stage 2: generating briefing with enriched content")
        draft = self._call_gemini(prompt)
        if not draft:
            logger.error("Stage 2: Gemini returned no content")
            return None
        if len(draft) < self._BRIEFING_MIN_CHARS:
            logger.warning(
                "Stage 2: briefing unusually short (%d chars < %d minimum)",
                len(draft), self._BRIEFING_MIN_CHARS,
            )

        # Stage 3: LLM-based refinement then deterministic post-processing
        refined = self._refine_briefing(draft)
        return self._post_process_briefing(refined)

    def _refine_briefing(self, draft: str) -> str:
        """Stage 3: Self-critique and refine the briefing for quality."""
        prompt = (
            "ä»¥ä¸‹ã®ãƒ‡ã‚¤ãƒªãƒ¼ãƒ–ãƒªãƒ¼ãƒ•ã‚£ãƒ³ã‚°ã®åŸç¨¿ã‚’æ ¡æ­£ãƒ»æ”¹å–„ã—ã¦ãã ã•ã„ã€‚\n\n"
            "## å“è³ªãƒã‚§ãƒƒã‚¯é …ç›®ï¼ˆä¸åˆæ ¼ãªã‚‰ä¿®æ­£ï¼‰\n"
            "1. ğŸ“ãƒªãƒ³ã‚¯ã®ãªã„ãƒˆãƒ”ãƒƒã‚¯ãŒã‚ã‚Œã°ã€ãã®ãƒˆãƒ”ãƒƒã‚¯ã‚’å‰Šé™¤ã™ã‚‹\n"
            "2. ä»¥ä¸‹ã®å®šå‹è¡¨ç¾ãŒã‚ã‚Œã°å…·ä½“çš„ãªè¡¨ç¾ã«æ›¸ãæ›ãˆã‚‹:\n"
            "   - ã€Œã€œã«æ³¨ç›®ãŒé›†ã¾ã£ã¦ã„ã¾ã™ã€â†’ å…·ä½“çš„ã«èª°ãŒä½•ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹\n"
            "   - ã€Œã€œãŒé‡è¦ã§ã™ã€â†’ ãªãœé‡è¦ã‹ã‚’å…·ä½“çš„ã«\n"
            "   - ã€Œæ³¨æ„ãŒå¿…è¦ã§ã™ã€â†’ å…·ä½“çš„ã«ä½•ã‚’ã™ã¹ãã‹\n"
            "   - ã€Œã€œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€â†’ æ ¹æ‹ ã‚’ç¤ºã—ã¦æ–­å®šã™ã‚‹ã‹å‰Šé™¤\n"
            "   - ã€Œä»Šå¾Œã®å‹•å‘ã«æ³¨ç›®ã€ã€Œå¼•ãç¶šãæ³¨è¦–ã€â†’ å‰Šé™¤ã™ã‚‹ã‹å…·ä½“çš„ãªæ—¥ä»˜ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã«ç½®æ›\n"
            "   - ã€Œã€œãŒæœŸå¾…ã•ã‚Œã¾ã™ã€â†’ èª°ãŒãªãœæœŸå¾…ã—ã¦ã„ã‚‹ã‹å…·ä½“çš„ã«\n"
            "3. åŒã˜èªå°¾ãŒ3å›ä»¥ä¸Šé€£ç¶šã—ã¦ã„ãŸã‚‰èªå°¾ã‚’å¤‰ãˆã‚‹\n"
            "4. 1ãƒˆãƒ”ãƒƒã‚¯ãŒ9è¡Œä»¥ä¸Šãªã‚‰8è¡Œä»¥å†…ã«å‰Šã‚‹\n"
            "5. **ãƒãƒ¼ã‚±ãƒƒãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³**: å…·ä½“çš„ãªæ•°å€¤ï¼ˆæŒ‡æ•°ã€%ã€ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼‰ãŒ1ã¤ã‚‚ãªã‘ã‚Œã°ã€\n"
            "   ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†’é ­ã«ã€Œãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼šè©²å½“è¨˜äº‹ã«å…·ä½“çš„æ•°å€¤ã®è¨˜è¼‰ãªã—ã€ã¨è¿½è¨˜\n"
            "6. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚»ã‚¯ã‚·ãƒ§ãƒ³**: å„é …ç›®ã«CVEç•ªå·ã¾ãŸã¯å…·ä½“çš„ãªã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢åãŒãªã‘ã‚Œã°ã€\n"
            "   ãã®é …ç›®ã‚’å‰Šé™¤ã™ã‚‹ã‹å…·ä½“åŒ–ã™ã‚‹\n"
            "7. ãƒã‚¤ãƒ©ã‚¤ãƒˆã¨ä»–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§åŒã˜è¨˜äº‹ï¼ˆåŒã˜URLï¼‰ã‚’æ‰±ã£ã¦ã„ãŸã‚‰ä»–ã‚»ã‚¯ã‚·ãƒ§ãƒ³å´ã‚’å‰Šé™¤\n"
            "8. å…·ä½“çš„äº‹å®Ÿï¼ˆæ•°å€¤ã€å›ºæœ‰åè©ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç­‰ï¼‰ã‚’1ã¤ã‚‚å«ã¾ãªã„ãƒãƒ¬ãƒƒãƒˆãƒã‚¤ãƒ³ãƒˆã¯å‰Šé™¤\n\n"
            "## ãƒ«ãƒ¼ãƒ«\n"
            "- Markdownã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ã¯ãã®ã¾ã¾ç¶­æŒã™ã‚‹\n"
            "- æƒ…å ±ã‚’è¿½åŠ ãƒ»æé€ ã—ãªã„ã€‚åŸç¨¿ã«ã‚ã‚‹æƒ…å ±ã ã‘ã§æ”¹å–„ã™ã‚‹\n"
            "- æ”¹å–„å¾Œã®ãƒ–ãƒªãƒ¼ãƒ•ã‚£ãƒ³ã‚°å…¨æ–‡ã®ã¿ã‚’å‡ºåŠ›ã™ã‚‹ã€‚èª¬æ˜ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã¯ä¸è¦\n\n"
            "## åŸç¨¿\n\n"
            f"{draft}"
        )
        logger.info("Stage 3: refining briefing quality")
        refined = self._call_gemini(prompt)
        return refined or draft

    # ------------------------------------------------------------------
    # Deterministic post-processing (no LLM calls)
    # ------------------------------------------------------------------

    _BANNED_PHRASES = [
        "æ³¨ç›®ãŒé›†ã¾ã£ã¦ã„ã¾ã™",
        "æ³¨ç›®ãŒé›†ã¾ã£ã¦",
        "ãŒé‡è¦ã§ã™",
        "ãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã™",
        "ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
        "æ³¨æ„ãŒå¿…è¦ã§ã™",
        "å¯¾ç­–ãŒæ€¥å‹™ã§ã™",
        "ãŒé€²ã‚“ã§ã„ã¾ã™",
        "ãŒåŠ é€Ÿã—ã¦ã„ã¾ã™",
        "ä»Šå¾Œã®å‹•å‘ã«æ³¨ç›®",
        "å¼•ãç¶šãæ³¨è¦–",
        "ãŒæœŸå¾…ã•ã‚Œã¾ã™",
        "ãŒè¦‹è¾¼ã¾ã‚Œã¾ã™",
    ]

    @staticmethod
    def _section_has_link(section_text: str) -> bool:
        """Check if a section contains at least one ğŸ“ markdown link."""
        return bool(re.search(r"ğŸ“\s*\[.*?\]\(https?://.*?\)", section_text))

    @staticmethod
    def _market_section_has_numbers(section_text: str) -> bool:
        """Check if market section contains actual numeric data."""
        return bool(re.search(
            r"\d+[,.]?\d*\s*%|"           # percentages like 3.5%
            r"(?:S&P|NASDAQ|æ—¥çµŒ|TOPIX|USD/JPY|ãƒ‰ãƒ«å††)\s*[\d,]+|"  # index values
            r"\$\s*[\d,]+|"                # dollar amounts
            r"[\d,]+\s*(?:å††|ãƒ‰ãƒ«|bps)",   # yen/dollar/bps amounts
            section_text,
        ))

    def _post_process_briefing(self, text: str) -> str:
        """Deterministic quality checks applied after LLM refinement."""
        sections = re.split(r"(^## .+$)", text, flags=re.MULTILINE)
        result_parts: list[str] = []
        banned_found: list[str] = []

        i = 0
        while i < len(sections):
            part = sections[i]
            # Check if this is a section header
            if part.startswith("## "):
                header = part
                body = sections[i + 1] if i + 1 < len(sections) else ""
                combined = header + body

                # Drop sections without links (except ä»Šå¾Œã®æ³¨ç›® which may not need them)
                if "ğŸ”®" not in header and not self._section_has_link(combined):
                    logger.warning(
                        "Post-process: dropping section without links: %s",
                        header.strip(),
                    )
                    i += 2
                    continue

                # Market section: inject data-insufficient notice if no numbers
                if "ãƒãƒ¼ã‚±ãƒƒãƒˆ" in header and not self._market_section_has_numbers(body):
                    body = "\nãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼šè©²å½“è¨˜äº‹ã«å…·ä½“çš„æ•°å€¤ã®è¨˜è¼‰ãªã—\n" + body
                    logger.info("Post-process: added data-insufficient notice to market section")

                result_parts.append(header)
                result_parts.append(body)
                i += 2
            else:
                result_parts.append(part)
                i += 1

        processed = "".join(result_parts)

        # Log banned phrases still present (for monitoring, not removal --
        # removing mid-sentence could break readability)
        for phrase in self._BANNED_PHRASES:
            count = processed.count(phrase)
            if count > 0:
                banned_found.append(f"'{phrase}' x{count}")
        if banned_found:
            logger.warning(
                "Post-process: banned phrases still present: %s",
                ", ".join(banned_found),
            )

        # Check for duplicate URLs across highlight and other sections
        highlight_urls: set[str] = set()
        in_highlight = False
        for line in processed.split("\n"):
            if "ğŸ”¥" in line and line.startswith("## "):
                in_highlight = True
            elif line.startswith("## "):
                in_highlight = False
            if in_highlight:
                for url_match in re.finditer(r"\(https?://[^\s)]+\)", line):
                    highlight_urls.add(url_match.group())

        if highlight_urls:
            dup_count = 0
            for url in highlight_urls:
                # Count occurrences outside highlight section
                all_occurrences = processed.count(url)
                if all_occurrences > 1:
                    dup_count += 1
            if dup_count:
                logger.warning(
                    "Post-process: %d URL(s) appear in both highlights and other sections",
                    dup_count,
                )

        return processed


def generate_briefing(articles: list[Article], api_key: str | None = None) -> str:
    """Generate a curated briefing. Returns empty string if no API key."""
    if not api_key:
        logger.warning("No API key provided, skipping briefing generation")
        return ""
    if not articles:
        logger.warning("No articles provided, skipping briefing generation")
        return ""
    summarizer = GeminiSummarizer(api_key=api_key)
    result = summarizer.generate_briefing(articles)
    if not result:
        logger.error(
            "Briefing generation failed after all retries for %d articles",
            len(articles),
        )
    return result or ""


def get_summarizer(api_key: str | None = None) -> Summarizer:
    """Factory: returns GeminiSummarizer if API key is available, else Passthrough."""
    if api_key:
        return GeminiSummarizer(api_key=api_key)
    return PassthroughSummarizer()
